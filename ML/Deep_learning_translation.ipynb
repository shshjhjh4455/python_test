{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aad898ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import unicodedata\n",
    "import re\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, GRU, Masking\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c78bbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1/1 [==============================] - 0s 395ms/step - loss: 4492.9731 - mse: 4492.9731\n",
      "Epoch 2/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 553.9543 - mse: 553.9543\n",
      "Epoch 3/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 69.2859 - mse: 69.2859\n",
      "Epoch 4/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.6495 - mse: 9.6495\n",
      "Epoch 5/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.3101 - mse: 2.3101\n",
      "Epoch 6/300\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.4054 - mse: 1.4054\n",
      "Epoch 7/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2925 - mse: 1.2925\n",
      "Epoch 8/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2771 - mse: 1.2771\n",
      "Epoch 9/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2736 - mse: 1.2736\n",
      "Epoch 10/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2717 - mse: 1.2717\n",
      "Epoch 11/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2699 - mse: 1.2699\n",
      "Epoch 12/300\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2682 - mse: 1.2682\n",
      "Epoch 13/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2665 - mse: 1.2665\n",
      "Epoch 14/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2648 - mse: 1.2648\n",
      "Epoch 15/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2631 - mse: 1.2631\n",
      "Epoch 16/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2614 - mse: 1.2614\n",
      "Epoch 17/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2598 - mse: 1.2598\n",
      "Epoch 18/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2582 - mse: 1.2582\n",
      "Epoch 19/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2565 - mse: 1.2565\n",
      "Epoch 20/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2549 - mse: 1.2549\n",
      "Epoch 21/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2533 - mse: 1.2533\n",
      "Epoch 22/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2517 - mse: 1.2517\n",
      "Epoch 23/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2502 - mse: 1.2502\n",
      "Epoch 24/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2486 - mse: 1.2486\n",
      "Epoch 25/300\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2471 - mse: 1.2471\n",
      "Epoch 26/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2455 - mse: 1.2455\n",
      "Epoch 27/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2440 - mse: 1.2440\n",
      "Epoch 28/300\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2425 - mse: 1.2425\n",
      "Epoch 29/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2410 - mse: 1.2410\n",
      "Epoch 30/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2395 - mse: 1.2395\n",
      "Epoch 31/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2380 - mse: 1.2380\n",
      "Epoch 32/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2366 - mse: 1.2366\n",
      "Epoch 33/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2351 - mse: 1.2351\n",
      "Epoch 34/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2337 - mse: 1.2337\n",
      "Epoch 35/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2323 - mse: 1.2323\n",
      "Epoch 36/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2308 - mse: 1.2308\n",
      "Epoch 37/300\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.2294 - mse: 1.2294\n",
      "Epoch 38/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2281 - mse: 1.2281\n",
      "Epoch 39/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2267 - mse: 1.2267\n",
      "Epoch 40/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2253 - mse: 1.2253\n",
      "Epoch 41/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2240 - mse: 1.2240\n",
      "Epoch 42/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2226 - mse: 1.2226\n",
      "Epoch 43/300\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2213 - mse: 1.2213\n",
      "Epoch 44/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2199 - mse: 1.2199\n",
      "Epoch 45/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2186 - mse: 1.2186\n",
      "Epoch 46/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2173 - mse: 1.2173\n",
      "Epoch 47/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2160 - mse: 1.2160\n",
      "Epoch 48/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2148 - mse: 1.2148\n",
      "Epoch 49/300\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2135 - mse: 1.2135\n",
      "Epoch 50/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2122 - mse: 1.2122\n",
      "Epoch 51/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2110 - mse: 1.2110\n",
      "Epoch 52/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2097 - mse: 1.2097\n",
      "Epoch 53/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2085 - mse: 1.2085\n",
      "Epoch 54/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2073 - mse: 1.2073\n",
      "Epoch 55/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2061 - mse: 1.2061\n",
      "Epoch 56/300\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2049 - mse: 1.2049\n",
      "Epoch 57/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2037 - mse: 1.2037\n",
      "Epoch 58/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2025 - mse: 1.2025\n",
      "Epoch 59/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2014 - mse: 1.2014\n",
      "Epoch 60/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2002 - mse: 1.2002\n",
      "Epoch 61/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1991 - mse: 1.1991\n",
      "Epoch 62/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1979 - mse: 1.1979\n",
      "Epoch 63/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1968 - mse: 1.1968\n",
      "Epoch 64/300\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.1956 - mse: 1.1956\n",
      "Epoch 65/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1945 - mse: 1.1945\n",
      "Epoch 66/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1934 - mse: 1.1934\n",
      "Epoch 67/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1923 - mse: 1.1923\n",
      "Epoch 68/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1913 - mse: 1.1913\n",
      "Epoch 69/300\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.1902 - mse: 1.1902\n",
      "Epoch 70/300\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.1891 - mse: 1.1891\n",
      "Epoch 71/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1881 - mse: 1.1881\n",
      "Epoch 72/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1870 - mse: 1.1870\n",
      "Epoch 73/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1860 - mse: 1.1860\n",
      "Epoch 74/300\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1849 - mse: 1.1849\n",
      "Epoch 75/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1839 - mse: 1.1839\n",
      "Epoch 76/300\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1829 - mse: 1.1829\n",
      "Epoch 77/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1819 - mse: 1.1819\n",
      "Epoch 78/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1809 - mse: 1.1809\n",
      "Epoch 79/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1799 - mse: 1.1799\n",
      "Epoch 80/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1789 - mse: 1.1789\n",
      "Epoch 81/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1779 - mse: 1.1779\n",
      "Epoch 82/300\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.1770 - mse: 1.1770\n",
      "Epoch 83/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1760 - mse: 1.1760\n",
      "Epoch 84/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1750 - mse: 1.1750\n",
      "Epoch 85/300\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.1741 - mse: 1.1741\n",
      "Epoch 86/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1732 - mse: 1.1732\n",
      "Epoch 87/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1722 - mse: 1.1722\n",
      "Epoch 88/300\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1713 - mse: 1.1713\n",
      "Epoch 89/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step - loss: 1.1704 - mse: 1.1704\n",
      "Epoch 90/300\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.1695 - mse: 1.1695\n",
      "Epoch 91/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1686 - mse: 1.1686\n",
      "Epoch 92/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1677 - mse: 1.1677\n",
      "Epoch 93/300\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1668 - mse: 1.1668\n",
      "Epoch 94/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1659 - mse: 1.1659\n",
      "Epoch 95/300\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.1651 - mse: 1.1651\n",
      "Epoch 96/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1642 - mse: 1.1642\n",
      "Epoch 97/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1634 - mse: 1.1634\n",
      "Epoch 98/300\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.1625 - mse: 1.1625\n",
      "Epoch 99/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1617 - mse: 1.1617\n",
      "Epoch 100/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1608 - mse: 1.1608\n",
      "Epoch 101/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1600 - mse: 1.1600\n",
      "Epoch 102/300\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1592 - mse: 1.1592\n",
      "Epoch 103/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1584 - mse: 1.1584\n",
      "Epoch 104/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1576 - mse: 1.1576\n",
      "Epoch 105/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1568 - mse: 1.1568\n",
      "Epoch 106/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1560 - mse: 1.1560\n",
      "Epoch 107/300\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.1552 - mse: 1.1552\n",
      "Epoch 108/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1544 - mse: 1.1544\n",
      "Epoch 109/300\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1536 - mse: 1.1536\n",
      "Epoch 110/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1529 - mse: 1.1529\n",
      "Epoch 111/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1521 - mse: 1.1521\n",
      "Epoch 112/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1513 - mse: 1.1513\n",
      "Epoch 113/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1506 - mse: 1.1506\n",
      "Epoch 114/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1498 - mse: 1.1498\n",
      "Epoch 115/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1491 - mse: 1.1491\n",
      "Epoch 116/300\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.1484 - mse: 1.1484\n",
      "Epoch 117/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1476 - mse: 1.1476\n",
      "Epoch 118/300\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.1469 - mse: 1.1469\n",
      "Epoch 119/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1462 - mse: 1.1462\n",
      "Epoch 120/300\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.1455 - mse: 1.1455\n",
      "Epoch 121/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1448 - mse: 1.1448\n",
      "Epoch 122/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1441 - mse: 1.1441\n",
      "Epoch 123/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1434 - mse: 1.1434\n",
      "Epoch 124/300\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.1427 - mse: 1.1427\n",
      "Epoch 125/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1421 - mse: 1.1421\n",
      "Epoch 126/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1414 - mse: 1.1414\n",
      "Epoch 127/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1407 - mse: 1.1407\n",
      "Epoch 128/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1400 - mse: 1.1400\n",
      "Epoch 129/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1394 - mse: 1.1394\n",
      "Epoch 130/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1387 - mse: 1.1387\n",
      "Epoch 131/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1381 - mse: 1.1381\n",
      "Epoch 132/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1374 - mse: 1.1374\n",
      "Epoch 133/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1368 - mse: 1.1368\n",
      "Epoch 134/300\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.1362 - mse: 1.1362\n",
      "Epoch 135/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1356 - mse: 1.1356\n",
      "Epoch 136/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1349 - mse: 1.1349\n",
      "Epoch 137/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1343 - mse: 1.1343\n",
      "Epoch 138/300\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.1337 - mse: 1.1337\n",
      "Epoch 139/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1331 - mse: 1.1331\n",
      "Epoch 140/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1325 - mse: 1.1325\n",
      "Epoch 141/300\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.1319 - mse: 1.1319\n",
      "Epoch 142/300\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.1313 - mse: 1.1313\n",
      "Epoch 143/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1307 - mse: 1.1307\n",
      "Epoch 144/300\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.1302 - mse: 1.1302\n",
      "Epoch 145/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1296 - mse: 1.1296\n",
      "Epoch 146/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1290 - mse: 1.1290\n",
      "Epoch 147/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1284 - mse: 1.1284\n",
      "Epoch 148/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1279 - mse: 1.1279\n",
      "Epoch 149/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1273 - mse: 1.1273\n",
      "Epoch 150/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1268 - mse: 1.1268\n",
      "Epoch 151/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1262 - mse: 1.1262\n",
      "Epoch 152/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1257 - mse: 1.1257\n",
      "Epoch 153/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1251 - mse: 1.1251\n",
      "Epoch 154/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1246 - mse: 1.1246\n",
      "Epoch 155/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1241 - mse: 1.1241\n",
      "Epoch 156/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1235 - mse: 1.1235\n",
      "Epoch 157/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1230 - mse: 1.1230\n",
      "Epoch 158/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1225 - mse: 1.1225\n",
      "Epoch 159/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1220 - mse: 1.1220\n",
      "Epoch 160/300\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1215 - mse: 1.1215\n",
      "Epoch 161/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1210 - mse: 1.1210\n",
      "Epoch 162/300\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.1205 - mse: 1.1205\n",
      "Epoch 163/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1200 - mse: 1.1200\n",
      "Epoch 164/300\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.1195 - mse: 1.1195\n",
      "Epoch 165/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1190 - mse: 1.1190\n",
      "Epoch 166/300\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.1185 - mse: 1.1185\n",
      "Epoch 167/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1180 - mse: 1.1180\n",
      "Epoch 168/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1176 - mse: 1.1176\n",
      "Epoch 169/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1171 - mse: 1.1171\n",
      "Epoch 170/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1166 - mse: 1.1166\n",
      "Epoch 171/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1161 - mse: 1.1161\n",
      "Epoch 172/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1157 - mse: 1.1157\n",
      "Epoch 173/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1152 - mse: 1.1152\n",
      "Epoch 174/300\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1148 - mse: 1.1148\n",
      "Epoch 175/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1143 - mse: 1.1143\n",
      "Epoch 176/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1139 - mse: 1.1139\n",
      "Epoch 177/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1134 - mse: 1.1134\n",
      "Epoch 178/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1130 - mse: 1.1130\n",
      "Epoch 179/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1126 - mse: 1.1126\n",
      "Epoch 180/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1121 - mse: 1.1121\n",
      "Epoch 181/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1117 - mse: 1.1117\n",
      "Epoch 182/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1113 - mse: 1.1113\n",
      "Epoch 183/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1109 - mse: 1.1109\n",
      "Epoch 184/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1104 - mse: 1.1104\n",
      "Epoch 185/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1100 - mse: 1.1100\n",
      "Epoch 186/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1096 - mse: 1.1096\n",
      "Epoch 187/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1092 - mse: 1.1092\n",
      "Epoch 188/300\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.1088 - mse: 1.1088\n",
      "Epoch 189/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1084 - mse: 1.1084\n",
      "Epoch 190/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1080 - mse: 1.1080\n",
      "Epoch 191/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1076 - mse: 1.1076\n",
      "Epoch 192/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1072 - mse: 1.1072\n",
      "Epoch 193/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1068 - mse: 1.1068\n",
      "Epoch 194/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1064 - mse: 1.1064\n",
      "Epoch 195/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1061 - mse: 1.1061\n",
      "Epoch 196/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1057 - mse: 1.1057\n",
      "Epoch 197/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1053 - mse: 1.1053\n",
      "Epoch 198/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1049 - mse: 1.1049\n",
      "Epoch 199/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1046 - mse: 1.1046\n",
      "Epoch 200/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1042 - mse: 1.1042\n",
      "Epoch 201/300\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1038 - mse: 1.1038\n",
      "Epoch 202/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1035 - mse: 1.1035\n",
      "Epoch 203/300\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1031 - mse: 1.1031\n",
      "Epoch 204/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1028 - mse: 1.1028\n",
      "Epoch 205/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1024 - mse: 1.1024\n",
      "Epoch 206/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1021 - mse: 1.1021\n",
      "Epoch 207/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1017 - mse: 1.1017\n",
      "Epoch 208/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1014 - mse: 1.1014\n",
      "Epoch 209/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1010 - mse: 1.1010\n",
      "Epoch 210/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1007 - mse: 1.1007\n",
      "Epoch 211/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1004 - mse: 1.1004\n",
      "Epoch 212/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1000 - mse: 1.1000\n",
      "Epoch 213/300\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.0997 - mse: 1.0997\n",
      "Epoch 214/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0994 - mse: 1.0994\n",
      "Epoch 215/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0991 - mse: 1.0991\n",
      "Epoch 216/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0987 - mse: 1.0987\n",
      "Epoch 217/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0984 - mse: 1.0984\n",
      "Epoch 218/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0981 - mse: 1.0981\n",
      "Epoch 219/300\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.0978 - mse: 1.0978\n",
      "Epoch 220/300\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0975 - mse: 1.0975\n",
      "Epoch 221/300\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.0972 - mse: 1.0972\n",
      "Epoch 222/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0969 - mse: 1.0969\n",
      "Epoch 223/300\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.0966 - mse: 1.0966\n",
      "Epoch 224/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0963 - mse: 1.0963\n",
      "Epoch 225/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0960 - mse: 1.0960\n",
      "Epoch 226/300\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0957 - mse: 1.0957\n",
      "Epoch 227/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0954 - mse: 1.0954\n",
      "Epoch 228/300\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0951 - mse: 1.0951\n",
      "Epoch 229/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0948 - mse: 1.0948\n",
      "Epoch 230/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0945 - mse: 1.0945\n",
      "Epoch 231/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0942 - mse: 1.0942\n",
      "Epoch 232/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0939 - mse: 1.0939\n",
      "Epoch 233/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0937 - mse: 1.0937\n",
      "Epoch 234/300\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0934 - mse: 1.0934\n",
      "Epoch 235/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0931 - mse: 1.0931\n",
      "Epoch 236/300\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0928 - mse: 1.0928\n",
      "Epoch 237/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0926 - mse: 1.0926\n",
      "Epoch 238/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0923 - mse: 1.0923\n",
      "Epoch 239/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0920 - mse: 1.0920\n",
      "Epoch 240/300\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0918 - mse: 1.0918\n",
      "Epoch 241/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0915 - mse: 1.0915\n",
      "Epoch 242/300\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.0912 - mse: 1.0912\n",
      "Epoch 243/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0910 - mse: 1.0910\n",
      "Epoch 244/300\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.0907 - mse: 1.0907\n",
      "Epoch 245/300\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0905 - mse: 1.0905\n",
      "Epoch 246/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0902 - mse: 1.0902\n",
      "Epoch 247/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0900 - mse: 1.0900\n",
      "Epoch 248/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0897 - mse: 1.0897\n",
      "Epoch 249/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0895 - mse: 1.0895\n",
      "Epoch 250/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0892 - mse: 1.0892\n",
      "Epoch 251/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0890 - mse: 1.0890\n",
      "Epoch 252/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0888 - mse: 1.0888\n",
      "Epoch 253/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0885 - mse: 1.0885\n",
      "Epoch 254/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0883 - mse: 1.0883\n",
      "Epoch 255/300\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0881 - mse: 1.0881\n",
      "Epoch 256/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0878 - mse: 1.0878\n",
      "Epoch 257/300\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0876 - mse: 1.0876\n",
      "Epoch 258/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0874 - mse: 1.0874\n",
      "Epoch 259/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0871 - mse: 1.0871\n",
      "Epoch 260/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0869 - mse: 1.0869\n",
      "Epoch 261/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0867 - mse: 1.0867\n",
      "Epoch 262/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0865 - mse: 1.0865\n",
      "Epoch 263/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0863 - mse: 1.0863\n",
      "Epoch 264/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0860 - mse: 1.0860\n",
      "Epoch 265/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0858 - mse: 1.0858\n",
      "Epoch 266/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0856 - mse: 1.0856\n",
      "Epoch 267/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0854 - mse: 1.0854\n",
      "Epoch 268/300\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.0852 - mse: 1.0852\n",
      "Epoch 269/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0850 - mse: 1.0850\n",
      "Epoch 270/300\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0848 - mse: 1.0848\n",
      "Epoch 271/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0846 - mse: 1.0846\n",
      "Epoch 272/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0844 - mse: 1.0844\n",
      "Epoch 273/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0842 - mse: 1.0842\n",
      "Epoch 274/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0840 - mse: 1.0840\n",
      "Epoch 275/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0838 - mse: 1.0838\n",
      "Epoch 276/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0836 - mse: 1.0836\n",
      "Epoch 277/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0834 - mse: 1.0834\n",
      "Epoch 278/300\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.0832 - mse: 1.0832\n",
      "Epoch 279/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0830 - mse: 1.0830\n",
      "Epoch 280/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0828 - mse: 1.0828\n",
      "Epoch 281/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0826 - mse: 1.0826\n",
      "Epoch 282/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0824 - mse: 1.0824\n",
      "Epoch 283/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0823 - mse: 1.0823\n",
      "Epoch 284/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0821 - mse: 1.0821\n",
      "Epoch 285/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0819 - mse: 1.0819\n",
      "Epoch 286/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0817 - mse: 1.0817\n",
      "Epoch 287/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0815 - mse: 1.0815\n",
      "Epoch 288/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0814 - mse: 1.0814\n",
      "Epoch 289/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0812 - mse: 1.0812\n",
      "Epoch 290/300\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0810 - mse: 1.0810\n",
      "Epoch 291/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0808 - mse: 1.0808\n",
      "Epoch 292/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.0807 - mse: 1.0807\n",
      "Epoch 293/300\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0805 - mse: 1.0805\n",
      "Epoch 294/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0803 - mse: 1.0803\n",
      "Epoch 295/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0802 - mse: 1.0802\n",
      "Epoch 296/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0800 - mse: 1.0800\n",
      "Epoch 297/300\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0798 - mse: 1.0798\n",
      "Epoch 298/300\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0797 - mse: 1.0797\n",
      "Epoch 299/300\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.0795 - mse: 1.0795\n",
      "Epoch 300/300\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.0793 - mse: 1.0793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23dfbc531c0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fuctional API로 회귀분석 구현\n",
    "X = [1,2,3,4,5,6,7,8,9]  # 공부 시간\n",
    "y = [11,22,33,44,53,66,77,87,95]   # 성적\n",
    "\n",
    "inputs = Input(shape=(1,))\n",
    "output = Dense(1, activation=\"linear\")(inputs)\n",
    "linear_model = Model(inputs, output)\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.01)   # 가중치 업데이트를 위한 확률적 경사하강법 객체 생성\n",
    "\n",
    "linear_model.compile(optimizer=sgd, loss=\"mse\", metrics=[\"mse\"])\n",
    "linear_model.fit(X,y,epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8058ab1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[75.55484]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model.predict([7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea982f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(tf.keras.Model) : \n",
    "    def __init__(self) : # 모델의 구조와 동적을 의미\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear_layer = tf.keras.layers.Dense(1, input_dim=1, activation=\"linear\")\n",
    "        \n",
    "    # 모델이 데이터를 입력받아 예측값을 리턴하는 포워드(forward) 연산 수행\n",
    "    def call(self, x) : \n",
    "        y_pred = self.linear_layer(x)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "954edb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2cb812c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhahn\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\jhahn\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\jhahn\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\jhahn\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\jhahn\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 808, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\jhahn\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer \"linear_regression\" (type LinearRegression).\n    \n    in user code:\n    \n        File \"C:\\Users\\jhahn\\AppData\\Local\\Temp\\ipykernel_9112\\1734274758.py\", line 7, in call  *\n            y_pred = self.linear_layer(x)\n        File \"C:\\Users\\jhahn\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"C:\\Users\\jhahn\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 227, in assert_input_compatibility\n            raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n    \n        ValueError: Input 0 of layer \"dense_2\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (None,)\n    \n    \n    Call arguments received:\n      • x=tf.Tensor(shape=(None,), dtype=int32)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m sgd \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mSGD(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)   \u001b[38;5;66;03m# 가중치 업데이트를 위한 확률적 경사하강법 객체 생성\u001b[39;00m\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39msgd, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1129\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[0;32m   1130\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\jhahn\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\jhahn\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\jhahn\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\jhahn\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 808, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\jhahn\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer \"linear_regression\" (type LinearRegression).\n    \n    in user code:\n    \n        File \"C:\\Users\\jhahn\\AppData\\Local\\Temp\\ipykernel_9112\\1734274758.py\", line 7, in call  *\n            y_pred = self.linear_layer(x)\n        File \"C:\\Users\\jhahn\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"C:\\Users\\jhahn\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 227, in assert_input_compatibility\n            raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n    \n        ValueError: Input 0 of layer \"dense_2\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (None,)\n    \n    \n    Call arguments received:\n      • x=tf.Tensor(shape=(None,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "sgd = tf.keras.optimizers.SGD(lr=0.01)   # 가중치 업데이트를 위한 확률적 경사하강법 객체 생성\n",
    "model.compile(optimizer=sgd, loss=\"mse\", metrics=[\"mse\"])\n",
    "model.fit(X,y,epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0748ef0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 개수 : 197463\n"
     ]
    }
   ],
   "source": [
    "lines = pd.read_csv(\"./datas/fra.txt\", names=[\"src\", \"tar\", \"lic\"], sep=\"\\t\")\n",
    "del lines[\"lic\"]\n",
    "print(\"전체 샘플의 개수 :\", len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3b551fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60,000개의 샘플만 가지고 기계 번역기를 구축\n",
    "lines = lines.loc[:, \"src\":\"tar\"]\n",
    "lines = lines[:60000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37b61fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11776</th>\n",
       "      <td>Tell everybody.</td>\n",
       "      <td>Dites-le à tout le monde.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10547</th>\n",
       "      <td>I just came in.</td>\n",
       "      <td>Je viens d'entrer.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39097</th>\n",
       "      <td>Tom remains puzzled.</td>\n",
       "      <td>Tom demeure perplexe.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42167</th>\n",
       "      <td>I can squeeze you in.</td>\n",
       "      <td>Je peux t'insérer.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18459</th>\n",
       "      <td>I came yesterday.</td>\n",
       "      <td>Je suis venu hier.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17709</th>\n",
       "      <td>Do you live here?</td>\n",
       "      <td>Résides-tu ici ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17857</th>\n",
       "      <td>Everyone escaped.</td>\n",
       "      <td>Tout le monde s'est évadé.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10454</th>\n",
       "      <td>I had a mishap.</td>\n",
       "      <td>J'ai eu une mésaventure.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59905</th>\n",
       "      <td>This is the guest room.</td>\n",
       "      <td>C'est la chambre d'amis.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39655</th>\n",
       "      <td>We're working on it.</td>\n",
       "      <td>On y travaille.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           src                         tar\n",
       "11776          Tell everybody.   Dites-le à tout le monde.\n",
       "10547          I just came in.          Je viens d'entrer.\n",
       "39097     Tom remains puzzled.       Tom demeure perplexe.\n",
       "42167    I can squeeze you in.          Je peux t'insérer.\n",
       "18459        I came yesterday.          Je suis venu hier.\n",
       "17709        Do you live here?            Résides-tu ici ?\n",
       "17857        Everyone escaped.  Tout le monde s'est évadé.\n",
       "10454          I had a mishap.    J'ai eu une mésaventure.\n",
       "59905  This is the guest room.    C'est la chambre d'amis.\n",
       "39655     We're working on it.             On y travaille."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfc695e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1956</th>\n",
       "      <td>I felt ill.</td>\n",
       "      <td>\\t Je me sentis malade. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14219</th>\n",
       "      <td>I know her well.</td>\n",
       "      <td>\\t Je la connais bien. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19111</th>\n",
       "      <td>I understand Tom.</td>\n",
       "      <td>\\t Je comprends Tom. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36980</th>\n",
       "      <td>I'm studying French.</td>\n",
       "      <td>\\t J'étudie le français. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31511</th>\n",
       "      <td>Put down the rifle.</td>\n",
       "      <td>\\t Pose le fusil. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26608</th>\n",
       "      <td>Tom giggled again.</td>\n",
       "      <td>\\t Tom a encore gloussé. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41539</th>\n",
       "      <td>He began with a joke.</td>\n",
       "      <td>\\t Il commença par une blague. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21222</th>\n",
       "      <td>Tom lost his job.</td>\n",
       "      <td>\\t Tom a perdu son emploi. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48777</th>\n",
       "      <td>He isn't happy at all.</td>\n",
       "      <td>\\t Il est loin d'être heureux. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1676</th>\n",
       "      <td>Are you in?</td>\n",
       "      <td>\\t Es-tu partant ? \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          src                                tar\n",
       "1956              I felt ill.         \\t Je me sentis malade. \\n\n",
       "14219        I know her well.          \\t Je la connais bien. \\n\n",
       "19111       I understand Tom.            \\t Je comprends Tom. \\n\n",
       "36980    I'm studying French.        \\t J'étudie le français. \\n\n",
       "31511     Put down the rifle.               \\t Pose le fusil. \\n\n",
       "26608      Tom giggled again.        \\t Tom a encore gloussé. \\n\n",
       "41539   He began with a joke.  \\t Il commença par une blague. \\n\n",
       "21222       Tom lost his job.      \\t Tom a perdu son emploi. \\n\n",
       "48777  He isn't happy at all.  \\t Il est loin d'être heureux. \\n\n",
       "1676              Are you in?              \\t Es-tu partant ? \\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.tar = lines.tar.apply(lambda x : \"\\t \" + x + \" \\n\")\n",
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9490893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자 집합 생성\n",
    "src_vocab = set()\n",
    "for line in lines.src :  # 1줄 씩 읽음\n",
    "    for char in line :   # 1개의 문자씩 읽음\n",
    "        src_vocab.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "440f3f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_vocab = set()\n",
    "for line in lines.tar :  # 1줄 씩 읽음\n",
    "    for char in line :   # 1개의 문자씩 읽음\n",
    "        tar_vocab.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a33a475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source 문장의 char 집합 : 79\n",
      "target 문장의 char 집합 : 105\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = len(src_vocab)+1\n",
    "tar_vocab_size = len(tar_vocab)+1\n",
    "print(\"source 문장의 char 집합 :\", src_vocab_size)\n",
    "print(\"target 문장의 char 집합 :\", tar_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df933a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 1, '!': 2, '\"': 3, '$': 4, '%': 5, '&': 6, \"'\": 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, '?': 23, 'A': 24, 'B': 25, 'C': 26, 'D': 27, 'E': 28, 'F': 29, 'G': 30, 'H': 31, 'I': 32, 'J': 33, 'K': 34, 'L': 35, 'M': 36, 'N': 37, 'O': 38, 'P': 39, 'Q': 40, 'R': 41, 'S': 42, 'T': 43, 'U': 44, 'V': 45, 'W': 46, 'X': 47, 'Y': 48, 'Z': 49, 'a': 50, 'b': 51, 'c': 52, 'd': 53, 'e': 54, 'f': 55, 'g': 56, 'h': 57, 'i': 58, 'j': 59, 'k': 60, 'l': 61, 'm': 62, 'n': 63, 'o': 64, 'p': 65, 'q': 66, 'r': 67, 's': 68, 't': 69, 'u': 70, 'v': 71, 'w': 72, 'x': 73, 'y': 74, 'z': 75, 'é': 76, '’': 77, '€': 78}\n",
      "{'\\t': 1, '\\n': 2, ' ': 3, '!': 4, '\"': 5, '$': 6, '%': 7, '&': 8, \"'\": 9, '(': 10, ')': 11, ',': 12, '-': 13, '.': 14, '0': 15, '1': 16, '2': 17, '3': 18, '4': 19, '5': 20, '6': 21, '7': 22, '8': 23, '9': 24, ':': 25, '?': 26, 'A': 27, 'B': 28, 'C': 29, 'D': 30, 'E': 31, 'F': 32, 'G': 33, 'H': 34, 'I': 35, 'J': 36, 'K': 37, 'L': 38, 'M': 39, 'N': 40, 'O': 41, 'P': 42, 'Q': 43, 'R': 44, 'S': 45, 'T': 46, 'U': 47, 'V': 48, 'W': 49, 'X': 50, 'Y': 51, 'Z': 52, 'a': 53, 'b': 54, 'c': 55, 'd': 56, 'e': 57, 'f': 58, 'g': 59, 'h': 60, 'i': 61, 'j': 62, 'k': 63, 'l': 64, 'm': 65, 'n': 66, 'o': 67, 'p': 68, 'q': 69, 'r': 70, 's': 71, 't': 72, 'u': 73, 'v': 74, 'w': 75, 'x': 76, 'y': 77, 'z': 78, '\\xa0': 79, '«': 80, '»': 81, 'À': 82, 'Ç': 83, 'É': 84, 'Ê': 85, 'Ô': 86, 'à': 87, 'â': 88, 'ç': 89, 'è': 90, 'é': 91, 'ê': 92, 'ë': 93, 'î': 94, 'ï': 95, 'ô': 96, 'ù': 97, 'û': 98, 'œ': 99, '\\u2009': 100, '\\u200b': 101, '‘': 102, '’': 103, '\\u202f': 104}\n"
     ]
    }
   ],
   "source": [
    "src_vocab = sorted(list(src_vocab))\n",
    "tar_vocab = sorted(list(tar_vocab))\n",
    "src_to_index = dict([word, i+1] for i, word in enumerate(src_vocab))\n",
    "tar_to_index = dict([word, i+1] for i, word in enumerate(tar_vocab))\n",
    "print(src_to_index)\n",
    "print(tar_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0dc1f67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source 문장의 정수 인코딩 : [[30, 64, 10], [30, 64, 10], [30, 64, 10], [30, 64, 10], [31, 58, 10]]\n"
     ]
    }
   ],
   "source": [
    "# 인덱스가 부여된 문자 집합으로부터 갖고 있는 훈련 데이터에 정수 인코딩을 수행\n",
    "encoder_input = []\n",
    "for line in lines.src : # 문장 단위로 처리하기 위한 반복문\n",
    "    encoded_line = []\n",
    "    for char in line : # 문자 단위로 처리하기 위한 반복문\n",
    "        encoded_line.append(src_to_index[char])\n",
    "        \n",
    "    encoder_input.append(encoded_line)\n",
    "    \n",
    "print(\"source 문장의 정수 인코딩 :\", encoder_input[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "702e0ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target 문장의 정수 인코딩 : [[1, 3, 48, 53, 3, 4, 3, 2], [1, 3, 39, 53, 70, 55, 60, 57, 14, 3, 2], [1, 3, 31, 66, 3, 70, 67, 73, 72, 57, 3, 4, 3, 2], [1, 3, 28, 67, 73, 59, 57, 3, 4, 3, 2], [1, 3, 45, 53, 64, 73, 72, 3, 4, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "# 인덱스가 부여된 문자 집합으로부터 갖고 있는 훈련 데이터에 정수 인코딩을 수행\n",
    "decoder_input = []\n",
    "for line in lines.tar : # 문장 단위로 처리하기 위한 반복문\n",
    "    decoded_line = []\n",
    "    for char in line : # 문자 단위로 처리하기 위한 반복문\n",
    "        decoded_line.append(tar_to_index[char])\n",
    "        \n",
    "    decoder_input.append(decoded_line)\n",
    "    \n",
    "print(\"target 문장의 정수 인코딩 :\", decoder_input[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f084bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target 문장 레이블의 정수 인코딩 : [[3, 48, 53, 3, 4, 3, 2], [3, 39, 53, 70, 55, 60, 57, 14, 3, 2], [3, 31, 66, 3, 70, 67, 73, 72, 57, 3, 4, 3, 2], [3, 28, 67, 73, 59, 57, 3, 4, 3, 2], [3, 45, 53, 64, 73, 72, 3, 4, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "# 디코더의 예측값과 비교하기 위한 실제값에 대해서 정수 인코딩을 수행\n",
    "decoder_target = []\n",
    "for line in lines.tar : \n",
    "    timestep = 0\n",
    "    encoded_line = []\n",
    "    for char in line : \n",
    "        if timestep > 0 : \n",
    "            encoded_line.append(tar_to_index[char])\n",
    "        timestep = timestep + 1\n",
    "    decoder_target.append(encoded_line)\n",
    "    \n",
    "print(\"target 문장 레이블의 정수 인코딩 :\", decoder_target[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ae83f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source 문장의 최대 길이 : 23\n",
      "target 문장의 최대 길이 : 76\n"
     ]
    }
   ],
   "source": [
    "# 패딩을 위한 영어, 프랑스 문장 샘플 길이 확인\n",
    "max_src_len = max([len(line) for line in lines.src])\n",
    "max_tar_len = max([len(line) for line in lines.tar])\n",
    "print(\"source 문장의 최대 길이 :\", max_src_len)\n",
    "print(\"target 문장의 최대 길이 :\", max_tar_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d901bb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = pad_sequences(encoder_input, maxlen=max_src_len, padding=\"post\")\n",
    "decoder_input = pad_sequences(decoder_input, maxlen=max_tar_len, padding=\"post\")\n",
    "decoder_target = pad_sequences(decoder_target, maxlen=max_tar_len, padding=\"post\")\n",
    "# 원-핫 인코딩\n",
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "00dc4d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "750/750 [==============================] - 235s 308ms/step - loss: 0.7281 - val_loss: 0.6560\n",
      "Epoch 2/40\n",
      "750/750 [==============================] - 227s 303ms/step - loss: 0.4489 - val_loss: 0.5318\n",
      "Epoch 3/40\n",
      "750/750 [==============================] - 253s 337ms/step - loss: 0.3781 - val_loss: 0.4712\n",
      "Epoch 4/40\n",
      "750/750 [==============================] - 235s 313ms/step - loss: 0.3363 - val_loss: 0.4336\n",
      "Epoch 5/40\n",
      "750/750 [==============================] - 238s 317ms/step - loss: 0.3083 - val_loss: 0.4100\n",
      "Epoch 6/40\n",
      "750/750 [==============================] - 244s 325ms/step - loss: 0.2877 - val_loss: 0.3932\n",
      "Epoch 7/40\n",
      "750/750 [==============================] - 248s 331ms/step - loss: 0.2718 - val_loss: 0.3819\n",
      "Epoch 8/40\n",
      "750/750 [==============================] - 223s 297ms/step - loss: 0.2590 - val_loss: 0.3716\n",
      "Epoch 9/40\n",
      "750/750 [==============================] - 252s 336ms/step - loss: 0.2481 - val_loss: 0.3659\n",
      "Epoch 10/40\n",
      "750/750 [==============================] - 279s 372ms/step - loss: 0.2388 - val_loss: 0.3608\n",
      "Epoch 11/40\n",
      "750/750 [==============================] - 210s 281ms/step - loss: 0.2306 - val_loss: 0.3583\n",
      "Epoch 12/40\n",
      "750/750 [==============================] - 214s 285ms/step - loss: 0.2233 - val_loss: 0.3560\n",
      "Epoch 13/40\n",
      "750/750 [==============================] - 229s 306ms/step - loss: 0.2166 - val_loss: 0.3555\n",
      "Epoch 14/40\n",
      "750/750 [==============================] - 243s 324ms/step - loss: 0.2108 - val_loss: 0.3521\n",
      "Epoch 15/40\n",
      "750/750 [==============================] - 261s 348ms/step - loss: 0.2051 - val_loss: 0.3538\n",
      "Epoch 16/40\n",
      "750/750 [==============================] - 233s 311ms/step - loss: 0.2002 - val_loss: 0.3540\n",
      "Epoch 17/40\n",
      "750/750 [==============================] - 241s 322ms/step - loss: 0.1955 - val_loss: 0.3528\n",
      "Epoch 18/40\n",
      "750/750 [==============================] - 230s 306ms/step - loss: 0.1910 - val_loss: 0.3526\n",
      "Epoch 19/40\n",
      "750/750 [==============================] - 248s 330ms/step - loss: 0.1870 - val_loss: 0.3546\n",
      "Epoch 20/40\n",
      "750/750 [==============================] - 235s 314ms/step - loss: 0.1831 - val_loss: 0.3556\n",
      "Epoch 21/40\n",
      "750/750 [==============================] - 246s 328ms/step - loss: 0.1795 - val_loss: 0.3556\n",
      "Epoch 22/40\n",
      "750/750 [==============================] - 236s 315ms/step - loss: 0.1760 - val_loss: 0.3585\n",
      "Epoch 23/40\n",
      "750/750 [==============================] - 244s 325ms/step - loss: 0.1729 - val_loss: 0.3594\n",
      "Epoch 24/40\n",
      "750/750 [==============================] - 237s 316ms/step - loss: 0.1698 - val_loss: 0.3619\n",
      "Epoch 25/40\n",
      "436/750 [================>.............] - ETA: 1:30 - loss: 0.1652"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 25>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m model \u001b[38;5;241m=\u001b[39m Model([encoder_inputs, decoder_inputs], decoder_outputs)\n\u001b[0;32m     23\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrmsprop\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mencoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_input\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m         \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1216\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1209\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1210\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1211\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1212\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1213\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1214\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1215\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1216\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1217\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1218\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:910\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    907\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 910\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    912\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    913\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:942\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    939\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    940\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    941\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 942\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    943\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3130\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3127\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   3128\u001b[0m   (graph_function,\n\u001b[0;32m   3129\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1959\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1955\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1957\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1958\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1959\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1960\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1961\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1962\u001b[0m     args,\n\u001b[0;32m   1963\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1964\u001b[0m     executing_eagerly)\n\u001b[0;32m   1965\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:598\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    597\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 598\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    604\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    605\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    606\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    607\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    610\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    611\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:58\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 58\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     61\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 훈련 과정에서, 이전 시점의 디코더 셀의 출력을 현재 시점의 디코더 셀의 입력으로\n",
    "# 넣어주지 않고, 이전 시점의 실제값을 현재 시점의 디코더 셀의 입력값으로 넣어주는\n",
    "# 방법(교사강요) 사용하여 seq2seq 모델을 설계\n",
    "encoder_inputs = Input(shape=(None, src_vocab_size))\n",
    "encoder_lstm = LSTM(units=256, return_state=True) # 인코더의 내부 상태 리턴\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)  # 인코더 출력, 은닉상태, 셀 상태 리턴\n",
    "encoder_states = [state_h, state_c]  # 컨텍스트 벡터\n",
    "\n",
    "decoder_inputs = Input(shape=(None, tar_vocab_size))\n",
    "decoder_lstm = LSTM(units=256, return_sequences=True, return_state=True)\n",
    "\n",
    "# 디코더에게 인코더의 은닉 상태, 셀 상태를 전달\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "# 디코더도 은닉 상태, 셀 상태를 리턴하지만 훈련 과정에서는 사용하지 않습니다\n",
    "decoder_softmax_layer = Dense(tar_vocab_size, activation=\"softmax\")\n",
    "\n",
    "# 출력층에 프랑스어의 단어 집합의 크기만큼 뉴런을 배치한 후\n",
    "# 소프트 맥스 함수를 사용하여 실제값과 오차를 구합니다.\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")\n",
    "\n",
    "model.fit(x=[encoder_input, decoder_input], y=decoder_target, batch_size=64,\n",
    "         epochs=40, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae6d667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 동작 단계\n",
    "# 1. 번역하고자 하는 입력 문장이 인코더에 들어가서 \n",
    "# 은닉 상태와 셀 상태를 얻습니다\n",
    "# 2. 인코더의 은닉 상태와 <SOS>에 해당하는 \\t를 디코더로 보냅니다\n",
    "# 디코더가 <EOS>에 해당하는 \\n이 나올 때까지 다음 문자를 예측하는 행동을 반복합니다\n",
    "\n",
    "# 인코더 정의\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)\n",
    "\n",
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs,\n",
    "                                                intial_state=decoder_states_inputs)\n",
    "\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, \n",
    "                     outputs=[decoder_outputs] + decoder_states)\n",
    "\n",
    "index_to_src = dict((i, char) for char, i in src_to_index.items())\n",
    "index_to_tar = dict((i, char) for char, i in tar_to_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89700a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스로부터 단어를 얻을 수 있는 함수 정의\n",
    "def decode_sequence(input_seq) : \n",
    "    states_value = encoder_model.predict(input_seq) # 입력으로부터 인코더의 상태를\n",
    "    \n",
    "    target_seq = np.zeros((1,1,tar_vocab_size))  # <SOS>에 해당하는 원-핫 벡터 생\n",
    "    target_seq[0, 0, tar_to_index[\"\\t\"]] = 1.\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    \n",
    "    while not stop_condition : \n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        \n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0,-1,:])\n",
    "        sampled_char = index_to_tar[sampled_token_index]\n",
    "        \n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += sampled_char\n",
    "        \n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단\n",
    "        if (sampled_char == \"\\n\" or len(decoded_sentence) > max_tar_len) : \n",
    "            stop_condition = True\n",
    "            \n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "        \n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "        \n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969f1fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq_index in [3,50,100,300,1001] : # 입력 문장의 인덱스\n",
    "    input_seq = encoder_input[seq_index:seq_index+1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    \n",
    "    print(35 * \"-\")\n",
    "    print(\"입력 문장 :\", lines.src[seq_index])\n",
    "    print(\"정답 문장 :\", lines.tar[seq_index][2:len(lines.tar[seq_index])-1])  # \"\\t\"\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ea5ec8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 단위 기계 번역기 구현\n",
    "num_samples = 60000    #약 19만개 데이터 중 60000개 샘플만 사용\n",
    "# 전처리 - 구두점 제거, 단어 구분\n",
    "def to_ascii(s) : \n",
    "    return \"\".join(c for c in unicodedata.normalize(\"NFD\",s)\n",
    "                  if unicodedata.category(c) != \"Mn\")\n",
    "\n",
    "def preprocess_sentence(sent) : \n",
    "    sent = to_ascii(sent.lower())\n",
    "    sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)  # 단어와 구두점 사이에 공백 추가\n",
    "    \n",
    "    # (a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고는 전부 공백으로 변환\n",
    "    sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
    "    \n",
    "    # 다수의 공백을 하나의 공백으로 치환\n",
    "    sent = re.sub(r\"\\s+\", \" \", sent)\n",
    "    \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d659d5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 전 영어 문장 : Have you had dinner?\n",
      "전처리 후 영어 문장 : have you had dinner ?\n",
      "전처리 전 프랑스어 문장 : Avez-vous déjà diné?\n",
      "전처리 후 프랑스어 문장 : avez vous deja dine ?\n"
     ]
    }
   ],
   "source": [
    "en_sent = u\"Have you had dinner?\"\n",
    "fr_sent = u\"Avez-vous déjà diné?\"\n",
    "print(\"전처리 전 영어 문장 :\", en_sent)\n",
    "print(\"전처리 후 영어 문장 :\", preprocess_sentence(en_sent))\n",
    "print(\"전처리 전 프랑스어 문장 :\", fr_sent)\n",
    "print(\"전처리 후 프랑스어 문장 :\", preprocess_sentence(fr_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f5c90e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 시 사용할 디코더의 입력 시퀀스와 실제값 분리\n",
    "# 입력 시퀀스에는 시작을 의미하는 토큰인 <sos>를 추가하고, 출력 시퀀스에는 종료를 의미하\n",
    "def load_preprocessed_data() : \n",
    "    encoder_input, decoder_input, decoder_target = [], [], []\n",
    "    \n",
    "    with open(\"./datas/fra.txt\", \"r\", encoding=\"utf8\") as lines : \n",
    "        for i, line in enumerate(lines) : \n",
    "            src_line, tar_line, _ = line.strip().split(\"\\t\")  # source와 target 분리\n",
    "            src_line = [w for w in preprocess_sentence(src_line).split()]\n",
    "            tar_line = preprocess_sentence(tar_line)\n",
    "            tar_line_in = [w for w in (\"<sos> \"+tar_line).split()]\n",
    "            tar_line_out = [w for w in (tar_line+\" <eos>\").split()]\n",
    "            encoder_input.append(src_line)\n",
    "            decoder_input.append(tar_line_in)\n",
    "            decoder_target.append(tar_line_out)\n",
    "            if i == num_samples - 1 : \n",
    "                break\n",
    "                \n",
    "    return encoder_input, decoder_input, decoder_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c937afa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_en_in, sents_fra_in, sents_fra_out = load_preprocessed_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "334a58e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 집합 생성, 정수 인코딩, 패딩\n",
    "tokenizer_en = Tokenizer(filters=\"\", lower=False)\n",
    "tokenizer_en.fit_on_texts(sents_en_in)\n",
    "encoder_input = tokenizer_en.texts_to_sequences(sents_en_in)\n",
    "encoder_input = pad_sequences(encoder_input, padding=\"post\")\n",
    "\n",
    "tokenizer_fra = Tokenizer(filters=\"\", lower=False)\n",
    "tokenizer_fra.fit_on_texts(sents_fra_in)\n",
    "tokenizer_fra.fit_on_texts(sents_fra_out)\n",
    "decoder_input = tokenizer_fra.texts_to_sequences(sents_fra_in)\n",
    "decoder_input = pad_sequences(decoder_input, padding=\"post\")\n",
    "decoder_target = tokenizer_fra.texts_to_sequences(sents_fra_out)\n",
    "decoder_target = pad_sequences(decoder_target, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8aca0148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코더의 입력의 크기(shape) : (60000, 9)\n",
      "디코더의 입력의 크기(shape) : (60000, 17)\n",
      "디코더의 레이블의 크기(shape) : (60000, 17)\n"
     ]
    }
   ],
   "source": [
    "print(\"인코더의 입력의 크기(shape) :\", encoder_input.shape)\n",
    "print(\"디코더의 입력의 크기(shape) :\", decoder_input.shape)\n",
    "print(\"디코더의 레이블의 크기(shape) :\", decoder_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cda2af13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어 집합의 크기 : 6678, 프랑스어 단어 집합의 크기 : 11362\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = len(tokenizer_en.word_index) + 1\n",
    "tar_vocab_size = len(tokenizer_fra.word_index) + 1\n",
    "print(\"영어 단어 집합의 크기 : {:d}, 프랑스어 단어 집합의 크기 : {:d}\".format(src_vocab_size, tar_vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f33c1226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어로부터 정수를 얻는 딕셔너리와 정수로부터 단어를 얻는 딕셔너리\n",
    "src_to_index = tokenizer_en.word_index\n",
    "index_to_src = tokenizer_en.index_word\n",
    "tar_to_index = tokenizer_fra.word_index\n",
    "index_to_tar = tokenizer_fra.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "abb9155e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 시퀀스 : [52473 52094 11620 ... 18531 56345 45077]\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터를 분리하기 전 데이터를 shuffle\n",
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print(\"랜덤 시퀀스 :\", indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5dfa28ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "51a95aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2200 3826    1    0    0    0    0    0    0]\n",
      "[   2 2207   32 5581    1    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0]\n",
      "[2207   32 5581    1    3    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input[40000])\n",
    "print(decoder_input[40000])\n",
    "print(decoder_target[40000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0a28db37",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_of_val = int(60000*0.1)  # 검증데이터 10%\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7e10c664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 source 데이터의 크기 : (54000, 9)\n",
      "훈련 target 데이터의 크기 : (54000, 17)\n",
      "훈련 target 레이블의 크기 : (54000, 17)\n",
      "테스트 source 데이터의 크기 : (6000, 9)\n",
      "테스트 target 데이터의 크기 : (6000, 17)\n",
      "테스트 target 레이블의 크기 : (6000, 17)\n"
     ]
    }
   ],
   "source": [
    "print(\"훈련 source 데이터의 크기 :\", encoder_input_train.shape)\n",
    "print(\"훈련 target 데이터의 크기 :\", decoder_input_train.shape)\n",
    "print(\"훈련 target 레이블의 크기 :\", decoder_target_train.shape)\n",
    "\n",
    "print(\"테스트 source 데이터의 크기 :\", encoder_input_test.shape)\n",
    "print(\"테스트 target 데이터의 크기 :\", decoder_input_test.shape)\n",
    "print(\"테스트 target 레이블의 크기 :\", decoder_target_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7b0a9eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 벡터의 차원과 LSTM의 은닉 상태의 크기를 64를 사용\n",
    "embedding_dim = 64\n",
    "hidden_units = 64\n",
    "\n",
    "# 인코더\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb = Embedding(src_vocab_size, embedding_dim)(encoder_inputs) # 임베딩 층\n",
    "enc_masking = Masking(mask_value=0.0)(enc_emb) # 패딩 0은 연산에서 제외\n",
    "encoder_lstm = LSTM(hidden_units, return_state=True)  # 상태값 리턴을 위해 return_state\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking)  # 은닉 상태와 셀 상태\n",
    "encoder_states = [state_h, state_c]  # 인코더의 은닉 상태와 셀 상태를 저장\n",
    "\n",
    "# 디코더\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(tar_vocab_size, hidden_units) # 임베딩 층\n",
    "dec_emb = dec_emb_layer(decoder_inputs)  # 패딩 0은 연산에서 제외\n",
    "dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
    "\n",
    "# 상태값 리턴을 위해 return_state는 True, 모든 시점에 대해서 단어를 예측하기 위해\n",
    "decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True)\n",
    "# 인코더의 은닉 상태를 초기 은닉 상태(initial_state)로 사용\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_masking,\n",
    "                                    initial_state=encoder_states)\n",
    "\n",
    "# 모든 시점의 결과에 대해서 소프트 맥스 함수를 사용한 출력층을 통해 단어 예측\n",
    "decoder_dense = Dense(tar_vocab_size, activation=\"softmax\")\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17d48d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "422/422 [==============================] - 193s 432ms/step - loss: 3.0530 - acc: 0.6060 - val_loss: 2.0547 - val_acc: 0.6120\n",
      "Epoch 2/50\n",
      "422/422 [==============================] - 190s 449ms/step - loss: 1.8436 - acc: 0.7029 - val_loss: 1.6897 - val_acc: 0.7371\n",
      "Epoch 3/50\n",
      "422/422 [==============================] - 175s 415ms/step - loss: 1.5834 - acc: 0.7454 - val_loss: 1.5102 - val_acc: 0.7575\n",
      "Epoch 4/50\n",
      "422/422 [==============================] - 173s 410ms/step - loss: 1.4145 - acc: 0.7717 - val_loss: 1.3668 - val_acc: 0.7841\n",
      "Epoch 5/50\n",
      "422/422 [==============================] - 174s 412ms/step - loss: 1.2819 - acc: 0.7944 - val_loss: 1.2583 - val_acc: 0.8009\n",
      "Epoch 6/50\n",
      "422/422 [==============================] - 173s 411ms/step - loss: 1.1820 - acc: 0.8069 - val_loss: 1.1830 - val_acc: 0.8094\n",
      "Epoch 7/50\n",
      "422/422 [==============================] - 171s 405ms/step - loss: 1.1036 - acc: 0.8158 - val_loss: 1.1213 - val_acc: 0.8166\n",
      "Epoch 8/50\n",
      "422/422 [==============================] - 170s 403ms/step - loss: 1.0369 - acc: 0.8231 - val_loss: 1.0711 - val_acc: 0.8220\n",
      "Epoch 9/50\n",
      "422/422 [==============================] - 166s 395ms/step - loss: 0.9776 - acc: 0.8297 - val_loss: 1.0271 - val_acc: 0.8279\n",
      "Epoch 10/50\n",
      "422/422 [==============================] - 169s 400ms/step - loss: 0.9234 - acc: 0.8355 - val_loss: 0.9891 - val_acc: 0.8326\n",
      "Epoch 11/50\n",
      "422/422 [==============================] - 171s 404ms/step - loss: 0.8744 - acc: 0.8409 - val_loss: 0.9556 - val_acc: 0.8367\n",
      "Epoch 12/50\n",
      "422/422 [==============================] - 171s 406ms/step - loss: 0.8297 - acc: 0.8456 - val_loss: 0.9263 - val_acc: 0.8396\n",
      "Epoch 13/50\n",
      "422/422 [==============================] - 170s 402ms/step - loss: 0.7886 - acc: 0.8501 - val_loss: 0.8998 - val_acc: 0.8426\n",
      "Epoch 14/50\n",
      "422/422 [==============================] - 171s 406ms/step - loss: 0.7508 - acc: 0.8543 - val_loss: 0.8791 - val_acc: 0.8452\n",
      "Epoch 15/50\n",
      "422/422 [==============================] - 177s 421ms/step - loss: 0.7161 - acc: 0.8582 - val_loss: 0.8567 - val_acc: 0.8482\n",
      "Epoch 16/50\n",
      "422/422 [==============================] - 165s 390ms/step - loss: 0.6831 - acc: 0.8622 - val_loss: 0.8394 - val_acc: 0.8504\n",
      "Epoch 17/50\n",
      "422/422 [==============================] - 160s 379ms/step - loss: 0.6524 - acc: 0.8663 - val_loss: 0.8224 - val_acc: 0.8527\n",
      "Epoch 18/50\n",
      "422/422 [==============================] - 161s 381ms/step - loss: 0.6232 - acc: 0.8698 - val_loss: 0.8067 - val_acc: 0.8552\n",
      "Epoch 19/50\n",
      "422/422 [==============================] - 163s 385ms/step - loss: 0.5960 - acc: 0.8736 - val_loss: 0.7942 - val_acc: 0.8569\n",
      "Epoch 20/50\n",
      "422/422 [==============================] - 161s 383ms/step - loss: 0.5703 - acc: 0.8773 - val_loss: 0.7799 - val_acc: 0.8586\n",
      "Epoch 21/50\n",
      "422/422 [==============================] - 164s 388ms/step - loss: 0.5459 - acc: 0.8808 - val_loss: 0.7708 - val_acc: 0.8602\n",
      "Epoch 22/50\n",
      "422/422 [==============================] - 166s 394ms/step - loss: 0.5232 - acc: 0.8843 - val_loss: 0.7608 - val_acc: 0.8619\n",
      "Epoch 23/50\n",
      "422/422 [==============================] - 168s 398ms/step - loss: 0.5014 - acc: 0.8878 - val_loss: 0.7514 - val_acc: 0.8631\n",
      "Epoch 24/50\n",
      " 24/422 [>.............................] - ETA: 2:30 - loss: 0.4633 - acc: 0.8964"
     ]
    }
   ],
   "source": [
    "model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \n",
    "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size=128, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787224a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq) : \n",
    "    # 입력으로부터 인코더의 마지막 시점의 상태(은닉 상태, 셀 상태)를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    \n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0,0] = tar_to_index[\"<sos>\"] # <sos>에 해당하는 정수 생성\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    \n",
    "    while not stop_condition : \n",
    "        # 이전 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        \n",
    "        sampled_token_index = np.argmax(output_tokens[0,-1,:])  # 예측 결과를 단어로 변환\n",
    "        sampled_char = index_to_tar[sampled_token_index]\n",
    "        \n",
    "        decoded_sentence += \" \"+sampled_char  # 현재 시점의 예측 단어를 예측 문장의 추가\n",
    "        \n",
    "        if (sampled_char == \"<eos>\" or len(decoded_sentence) > 50) : \n",
    "            stop_condition = True\n",
    "            \n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0,0] = sampled_token_index\n",
    "        \n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a569f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영어 문장에 해당하는 정수 시퀀스를 입력 받으면 정수로부터\n",
    "# 영어 단어를 리턴하는 index_to_src를 통해 영어 문장으로 변환하는 함수\n",
    "def seq_to_src(input_seq) : \n",
    "    sentence = \"\"\n",
    "    for encoded_word in input_seq : \n",
    "        if (encoded_word != 0) : \n",
    "            sentence = sentence + index_to_src[encoded_word] + \" \"\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3396505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq_to_tar(input_seq) : \n",
    "    sentence = \"\"\n",
    "    for encoded_word in input_seq : \n",
    "        if (encoded_word != 0 and encoded_word != tar_to_index[\"<sos>\"] and encoded_word != tar_toe_index[\"<eos>\"]) : \n",
    "            sentence = sentence + index_to_tar[encoded_word] + \" \"\n",
    "            \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b09a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq_index in [3, 50, 100, 300, 1001] : \n",
    "    input_seq = encoder_input_train[seq_index : seq_index+1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    \n",
    "    print(\"입력문장 :\",seq_to_src(encoder_input_train[seq_index]))\n",
    "    print(\"정답문장 :\",seq_to_tar(decoder_input_train[seq_index]))\n",
    "    print(\"번역문장 :\",decoded_sentence[1:-5])\n",
    "    print(\"-\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('baseDeep')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d5bd3b74290b6fecca2d77c6682b8ba7e9275f0a56c500dd407ba5b0bc3fc494"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
